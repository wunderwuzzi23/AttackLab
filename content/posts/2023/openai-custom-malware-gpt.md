---
title: "Introducing GPTs that steal your data"
date: 2023-12-07T16:50:49-08:00
draft: true
tags: [
     "aiml", "machine learning","ai injections","ttp","llm"
    ]

twitter:
  card: "summary_large_image"
  site: "@wunderwuzzi23"
  creator: "@wunderwuzzi23"
  title: "OpenAI GPTs that steal your data: Welcome MalwareGPT"
  description: "Custom GPTs can contain malicious objectives, like trying to scam users or actively exfiltrate data. This posts describes the first ever public MalwareGPT."
  image: "https://embracethered.com/blog/images/2023/malwaregpt.png"
---

When OpenAI released [GPTs](https://openai.com/blog/introducing-gpts) last month I had plans for an interesting GPT.

## Malicious ChatGPT Agents

The idea was to create MalwareGPT that forwards users' chat messages to a third party server. It also asks users for personal information like emails and passwords.

### Why would this be possible end to end?

[ChatGPT cannot keep your conversation private or confidential](/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/), because it loads images from any website. **This allows data to be sent to a third party server.**

A GPT can contain instructions that ask the user for information and simply sends it elsewhere. A demo proof-of-concept GPT was quickly created.

[![data gone gpt - demo](/blog/images/2023/malwaregpt-small.png)](/blog/images/2023/malwaregpt.png)

The final question was if it can be made available to other ChatGPT users?

### Publishing the malicious GPT

When creating a GPT it is a private GPT and can only be used by it's creator. There are three publishing settings:
1. Only me (private)
2. Anyone with a link
3. Public

When I first switched the setting to Public, I got this error:

[![MalwareGPT Sharing](/blog/images/2023/malwaregpt-not-ready-for-sharing-error.png)](/blog/images/2023/malwaregpt-not-ready-for-sharing-error.png)

OpenAI blocks publishing of certain GPTs. The first version contained words such as "data exfiltration" and "malicious" and it apparently failed "brand and usage" guidelines.  

### The quick fix 

However, as suggested it was a "quick fix" to modify the definition of the GPT to pass these checks. Afterwards, it was published for anyone to use! 

There is a note that the GPT might appear in the GPT store:
[![MalwareGPT in store](/blog/images/2023/malwaregpt-may-appear-in-store.png)](/blog/images/2023/malwaregpt-may-appear-in-store.png)

**Automatic malware proliferation! Scary!**

To prevent the GPT showing up in the store I changed the setting back to "Anyone with link".

## Demo Video

If you read that far I'm sure you want to see the demo:

{{< youtube FYKX_Eyj3ek >}}

<span> </span>

Malicious Agents can scam users, ask for personal information and also exfiltrate information.

**This should be pretty alarming.**

## Disclosure

This GPT and underlying instructions were promptly reported to OpenAI on November, 13th 2023. However, the ticket was closed on November 15th as "Not Applicable". Two follow up inquiries remained unanswered. Hence it seems best to share this with the public to raise awareness. 

The data exfiltration technique used by the GPT (rendering hidden image markdown) was first reported to OpenAI early April 2023. 

## Conclusions

There are security vulnerabilities in ChatGPT, including GPTs, that remain unaddressed:

1. **GPTs can be designed as malicious agents** that try to scam users
2. **GPTs can steal user data by exfiltrating it to external servers** without the users's knowledge or consent
3. **It is trivial to bypass the current validation checks** for publishing a  malicious GPT to the world

**Because of this vulnerability, OpenAI cannot guarantee the privacy and confidentiality of your ChatGPT conversations. This includes, but is not limited to custom GPTs.**

[Bing Chat](/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/), [Google Bard](/blog/posts/2023/google-bard-data-exfiltration/), Anthropic Claude and many others fixed such issues after being made aware of them. Let's hope fixes get implemented, so we can safely enjoy using GPTs.

Thanks for reading.

## Appendix

### Chaos Communication Congress

I will be at the [37th Chaos Communication Congress](https://events.ccc.de/en/category/37c3/) speaking about security issues like this, so come by and check out my talk "NEW IMPORTANT INSTRUCTIONS".


### Demo Screenshot

[![screenshot](/blog/images/2023/datagone-gpt-exfil-inprogress.png)](/blog/images/2023/datagone-gpt-exfil-inprogress.png)

### References

* [Video: Data Exfiltration Vulnerabilities in LLM apps](https://www.youtube.com/watch?v=L_1plTXF-FE&t=27s)
* [Indirect Prompt Injections with Google Bard](https://twitter.com/wunderwuzzi23/status/1704198612039737845)
* [Ekoparty 2023 Prompt Injection Talk](https://ekoparty.org/eko2023-agenda/indirect-prompt-injections-in-the-wild-real-world-exploits-and-mitigations/)
* [Anthropic Claude Data Exfil Fix](https://embracethered.com/blog/posts/2023/anthropic-fixes-claude-data-exfiltration-via-images/)
* [Microsoft Bing Chat Data Exfiltration Fix](/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/) 
* [ChatGPT Markdown Injection to Data Exfil Vulnerability](/blog/posts/2023/chatgpt-webpilot-data-exfil-via-markdown-injection/) 
* [37c3 - unlocked](https://events.ccc.de/en/category/37c3/)
