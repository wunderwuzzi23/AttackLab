<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai injection on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/ai-injection/</link>
    <description>Recent content in ai injection on Embrace The Red</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2024</copyright>
    <lastBuildDate>Wed, 22 May 2024 12:24:07 -0700</lastBuildDate><atom:link href="https://embracethered.com/blog/tags/ai-injection/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ChatGPT: Hacking Memories with Prompt Injection</title>
      <link>https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/</link>
      <pubDate>Wed, 22 May 2024 12:24:07 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2024/chatgpt-hacking-memories/</guid>
      <description>OpenAI recently introduced a memory feature in ChatGPT, enabling it to recall information across sessions, creating a more personalized user experience.
However, with this new capability comes risks. Imagine if an attacker could manipulate your AI assistant (chatbot or agent) to remember false information, bias or even instructions. This is not a futuristic scenario, the attack that makes this possible is called Indirect Prompt Injection.
In this post we will explore how memory features might be exploited by looking at three different attack avenues: Connected Apps, Uploaded Documents (Images) and Browsing.</description>
    </item>
    
    <item>
      <title>Bobby Tables but with LLM Apps - Google NotebookLM Data Exfiltration</title>
      <link>https://embracethered.com/blog/posts/2024/google-notebook-ml-data-exfiltration/</link>
      <pubDate>Mon, 15 Apr 2024 08:11:30 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2024/google-notebook-ml-data-exfiltration/</guid>
      <description>Google&amp;rsquo;s NotebookLM is an experimental project that was released last year. It allows users to upload files and analyze them with a large language model (LLM).
However, it is vulnerable to Prompt Injection, meaning that uploaded files can manipulate the chat conversation and control what the user sees in responses.
There is currently no known solution to these kinds of attacks, so users can&amp;rsquo;t implicitly trust responses from large language model applications when untrusted data is involved.</description>
    </item>
    
    <item>
      <title>Google AI Studio Data Exfiltration via Prompt Injection - Possible Regression and Fix</title>
      <link>https://embracethered.com/blog/posts/2024/google-aistudio-mass-data-exfil/</link>
      <pubDate>Sun, 07 Apr 2024 16:00:30 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2024/google-aistudio-mass-data-exfil/</guid>
      <description>What I like about the rapid advancements and excitement about AI over the last few years is that we see a resurgence of the testing discipline!
Software testing is hard, and adding AI to the mix does not make it easier at all!
Google AI Studio - Initially not vulnerable to data leakage via image rendering When Google released AI Studio last year I checked for the common image markdown data exfiltration vulnerability and it was not vulnerable.</description>
    </item>
    
  </channel>
</rss>
