<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plugins on Embrace The Red</title>
    <link>https://embracethered.com/blog/tags/plugins/</link>
    <description>Recent content in plugins on Embrace The Red</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>(c) WUNDERWUZZI 2018-2023</copyright>
    <lastBuildDate>Thu, 06 Jul 2023 16:30:00 -0700</lastBuildDate><atom:link href="https://embracethered.com/blog/tags/plugins/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>OpenAI Removes the &#34;Chat with Code&#34; Plugin From Store</title>
      <link>https://embracethered.com/blog/posts/2023/chatgpt-chat-with-code-plugin-take-down/</link>
      <pubDate>Thu, 06 Jul 2023 16:30:00 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2023/chatgpt-chat-with-code-plugin-take-down/</guid>
      <description>In the previous post we discussed the risks of OAuth enabled plugins being commonly vulnerable to Cross Plugin Request Forgery and how OpenAI is seemingly not enforcing new plugin store policies. As an example we explored how the &amp;ldquo;Chat with Code&amp;rdquo; plugin is vulnerable.
Recently, a post on Reddit titled &amp;ldquo;This is scary! Posting stuff by itself&amp;rdquo; shows how a conversation with ChatGPT, out of the blue (and what appears to be by accident) created a Github Issue!</description>
    </item>
    
    <item>
      <title>Plugin Vulnerabilities: Visit a Website and Have Your Source Code Stolen</title>
      <link>https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/</link>
      <pubDate>Tue, 20 Jun 2023 08:00:22 -0700</pubDate>
      
      <guid>https://embracethered.com/blog/posts/2023/chatgpt-plugin-vulns-chat-with-code/</guid>
      <description>OpenAI continues to add plugins with security vulnerabilities to their store.
In particular powerful plugins that can impersonate a user are not getting the required security scrutiny, or a general mitigation at the platform level.
As a brief reminder, one of the challenges Large Language Model (LLM) User-Agents, like ChatGPT, and plugins face is the Confused Deputy Problem / Plugin Request Forgery Attacks, which means that during a Prompt Injection attack an adversary can issue commands to plugins to cause harm.</description>
    </item>
    
  </channel>
</rss>
